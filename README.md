-In urban cities, as traditional management systems fail to adapt to dynamic, real-time conditions, there is an urgent need for an adaptive traffic control system to reduce the urban traffic congestion problem, which tends to problems like longer commutes, more pollution and higher fuel costs. This research paper includes an investigation of the application of Deep Reinforcement Learning (DRL) frameworks, integrated with Vehicle-to-Infrastructure (V2I) communication, to create a real-time adaptive and efficient traffic control system. A central approach involves deploying multi-agent systems (MADRL) in which traffic signals or roadside units (RSUs) act as agents to optimize network-wide smooth traffic flow. With the help of real-time data like vehicle density, speed, and waiting times collected by V2I communication, system can adapt according to the real-time conditions. The proposed research successfully shows how the traffic control system achieves a 66% increase in traffic throughput and a 59% decrease in idle queue lengths, with 43% reduction in travel time and a 33% drop in fuel consumption. The findings demonstrate how MADRL, combined with V2I communication, can be used to achieve approximately zero failure rates, proving their strong robustness and scalability for future smart city infrastructures.
